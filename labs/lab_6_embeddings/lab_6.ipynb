{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6: Word2Vec from scratch in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we'll write Word2Vec, a particularly popular, simple, and powerful model for unsupervised learning of embedding vectors for words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Word2Vec\n",
    "Word2Vec trains embeddings by using them as input to a logistic regression model, trained to predict a word given nearby words.\n",
    "\n",
    "Alternatively, you can think of Word2Vec as a neural network with linear activations and one hidden layer, trained to predict a word given nearby words.\n",
    "In this interpretation, the input and output of the model are both one-hot encoded vectors, and the embeddings of the words are the activations of the hidden layer given that word's one-hot encoding as input.\n",
    "Since the inputs are one-hot encoded, it's more efficient (but equivalent) to use an embedding lookup instead of a matrix multiply for the first layer.\n",
    "\n",
    "The data is produced by taking any large body of text (e.g. Wikipedia) and creating (context, target) pairs, where the target is any word and the context is the $n$ words to its left and right.\n",
    "\n",
    "There are two common ways of training Word2Vec:\n",
    " - The \"skip-gram\" model uses the target word as input and predicts context words.\n",
    " - The \"continuous bag-of-words\" (CBOW) model uses the context words as input and predicts the target word\n",
    " \n",
    "We'll focus on the CBOW model, since it tends to work better for small datasets.\n",
    "\n",
    "The process of generating the data for CBOW is as follows:\n",
    " 1. Tokenize (convert words, which are strings, to integer \"tokens\" indicating the word) the dataset by assigning each word a unique integer (integer encoding)\n",
    " 2. For each word in the dataset, create a single (`context`, `target`) pair, where `target` is the integer encoding of the word and `context` is a list of the integer encodings of the $n=1$ words to the left and right of the target word\n",
    "\n",
    "(I've already written the code to do this for you below)\n",
    " \n",
    "Then, the model is trained to predict the one-hot encoding of the target word given the integer encodings of the context words:\n",
    " 1. For each context word, look up its embedding in a table\n",
    " 2. Combine these into an \"average context embedding,\" which is the depth-wise average of the embeddings of the individual context words. This results in a single embedding vector, which acts as the \"total context\" in some sense.\n",
    " 3. Perform a logistic regression (equivalently, a single dense layer with softmax activation) to predict the target word using the average context embedding as input.\n",
    "\n",
    "Instead of full logistic regression, which uses the softmax function over all of the many words that appear in the dataset, we will use candidate sampling (specifically, noise-contrastive estimation) to compute the loss.\n",
    "This should speed up training significantly.\n",
    "\n",
    "For more info on Word2Vec, see [TensorFlow's \"Vector Representations of Words\" tutorial](https://www.tensorflow.org/tutorials/representation/word2vec), or Alex Minnaar's two tutorials, [one on the skip-gram model](http://alexminnaar.com/author/word2vec-tutorial-part-i-the-skip-gram-model.html) and [the other on the CBOW model](http://alexminnaar.com/author/word2vec-tutorial-part-ii-the-continuous-bag-of-words-model.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Download and preprocess the data\n",
    "The dataset is the Cornell Movie-Dialogs Corpus, a collection of dialogue from movie scripts.\n",
    "Download it from the link [here](http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html), then unzip it in a subfolder called \"data\".\n",
    "Your directory tree should include the file `./data/cornell movie-dialogs corpus/movie_lines.txt`.\n",
    "\n",
    "I've written all the code for loading and preprocessing the data below, but read through it to understand what's going on.\n",
    "You'll need to use parts of it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1: Read the data\n",
    "First, we read the lines as separate strings from the text file.\n",
    "\n",
    "If you are running into memory problems (i.e. running out of RAM while working on this assignment) you can increase `skip_header` to train on a smaller dataset (but your resulting embeddings will be less good).\n",
    "There are about 300,000 lines total, so you can set `skip_header` to skip some percentage of those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings on incorrectly-formatted inputs\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "_, _, _, _, lines = np.genfromtxt(\n",
    "    './data/cornell movie-dialogs corpus/movie_lines.txt',\n",
    "    dtype='<U128', \n",
    "    delimiter='+++$+++', autostrip=True,\n",
    "    encoding='latin1', invalid_raise=False, unpack=True,\n",
    "    skip_header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2: Tokenize\n",
    "Keras has a built-in tokenization utility, which we'll use.\n",
    "This creates two dictionaries, which you'll need to use later:\n",
    " - `tokenizer.word_index` maps from string words to integer tokens\n",
    " - `tokenizer.index_word` maps from integer tokens to string words\n",
    "\n",
    "Then, we convert the dialogue lines into lists of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The integer encoding of \"yes\" is: 67\n",
      "The word with integer encoding 10 is: what\n"
     ]
    }
   ],
   "source": [
    "# Create a tokenizer and assign each word an integer\n",
    "tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "\n",
    "# Convert the lines to lists of integers\n",
    "tokenized_lines = tokenizer.texts_to_sequences(lines)\n",
    "\n",
    "# Delete the original lines to save memory\n",
    "del(lines)\n",
    "\n",
    "# Here's an example of how to use word_index and index_word\n",
    "print('The integer encoding of \"yes\" is:',\n",
    "      tokenizer.word_index['yes'])\n",
    "print('The word with integer encoding 10 is:',\n",
    "      tokenizer.index_word[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3: Create (context, target) pairs\n",
    "In this case, \"target\" means the integer encoding of a word, and \"context\" means a list of the integer encodings of the words to its left and right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_words: 54034\n",
      "n_examples: 1178349\n"
     ]
    }
   ],
   "source": [
    "window_size = 1  # Consider 1 word on each side of target\n",
    "stride = 2       # Avoid window overlap\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "# Add (context, target) pairs to the dataset\n",
    "for line in tokenized_lines:\n",
    "    # Do not use lines that are too short\n",
    "    if len(line) < 2 * window_size + 1:\n",
    "        continue\n",
    "    \n",
    "    for target_idx in range(window_size, \n",
    "                            len(line) - window_size, \n",
    "                            stride):\n",
    "        target = line[target_idx]\n",
    "        left_context = line[target_idx - window_size : \n",
    "                            target_idx]\n",
    "        right_context = line[target_idx + 1 :\n",
    "                             target_idx + window_size + 1]\n",
    "        \n",
    "        Y.append(target)\n",
    "        X.append(left_context + right_context)\n",
    "\n",
    "# Convert to ndarrays\n",
    "X = np.asarray(X)\n",
    "Y = np.asarray(Y)\n",
    "\n",
    "# These constants may be useful for you later\n",
    "n_words = max(tokenizer.word_index.values())\n",
    "n_examples = len(X)\n",
    "\n",
    "print('n_words:', n_words)\n",
    "print('n_examples:', n_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4: Make a TSV metadata file\n",
    "This creates a metadata file in the format that the TensorBoard Projector uses (tab-separated values).\n",
    "This will let us see the names for words later when we visualize the embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Make a logs directory if none exists yet\n",
    "os.makedirs('./logs', exist_ok=True)\n",
    "\n",
    "# Make TSV metadata file\n",
    "with open('./logs/metadata.tsv', 'w', encoding = 'utf-8') as f:\n",
    "    # Header specifies column name\n",
    "    f.write('Word\\tIndex\\tCount\\n')\n",
    "    \n",
    "    # Unrecognized values have an integer encoding of 0\n",
    "    f.write('UNK\\t0\\t0\\n')\n",
    "    \n",
    "    # One word per line\n",
    "    for i in range(1, n_words):\n",
    "        word = tokenizer.index_word[i]\n",
    "        count = tokenizer.word_counts[word]\n",
    "        f.write('{}\\t{}\\t{}\\n'.format(word, count, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.5: Build a TensorFlow data pipeline\n",
    "I've set up the `tf.data.Dataset` and the `tf.summary.SummaryWriter` for you.\n",
    "Feel free to change the training hyperparameters and transforms if you like.\n",
    "\n",
    "You might want to try:\n",
    " - Changing the batch size to suit your RAM / GPU memory situation\n",
    " - Removing `.cache()` from the transforms if you can't fit the whole dataset in RAM\n",
    " - Changing n_negative_samples. Increasing it makes computing the loss slower but more accurate per batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches per epoch: 9205.8515625\n"
     ]
    }
   ],
   "source": [
    "# Training hyperparameters\n",
    "n_epochs = 10\n",
    "batch_size = 128\n",
    "n_batches_per_epoch = n_examples / batch_size\n",
    "print('Batches per epoch:', n_batches_per_epoch)\n",
    "\n",
    "# Number of wrong words to sample when computing\n",
    "# the loss using noise-contrastive estimation.\n",
    "n_negative_samples = 128\n",
    "\n",
    "# Construct dataset and apply transforms\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X, Y)) \\\n",
    "    .shuffle(1000) \\\n",
    "    .batch(batch_size) \\\n",
    "    .cache() \\\n",
    "    .repeat(n_epochs)\n",
    "\n",
    "writer = tf.summary.create_file_writer('./logs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Build a model graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0: Model hyperparameters\n",
    "`embedding_size` is the number of dimensions in the embedding vector of each word.\n",
    "Try to train 64-dimensional embeddings, but if training takes too long, feel free to reduce this to 32 or 16.\n",
    "\n",
    "`validation_words` is a list of words we'll use to see how good our embeddings are.\n",
    "While training, we'll periodically print out the words that have embeddings most similar to these words.\n",
    "As training progresses, this should start printing words with similar meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 64\n",
    "validation_words = ['yes', 'small', 'thousand']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Word2Vec class\n",
    "\n",
    "Write a `tf.Module` class called `Word2Vec`. We are concerned with a tensor which holds the context integer encodings (shape: `(batch_size, 2)`) and a tensor which holds the target integer encoding (shape: `(batch_size,)`) which will be our x and y variables respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1: Embedding variable\n",
    "Make a variable which holds the embeddings for each of the words.\n",
    "It should be a rank-2 tensor (a matrix) where the $i$-th row is the embedding vector for the word with integer encoding $i$.\n",
    "Its shape should be `(n_words, embedding_size)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2: Create the variables for logistic regression\n",
    "Create weight and bias variables for a logistic regression which takes in the average context embedding and outputs a probability for each word.\n",
    "\n",
    "NOTE: We won't actually ever compute the output of this logistic regression by hand (i.e. the matrix multiplication and softmax activation), since we're using noise-contrastive estimation to approximate it.\n",
    "TensorFlow's `nce_loss` function just takes the weights and bias tensors as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3: Compute the average context embedding\n",
    "Look up the embeddings of each of the context words (with a single call to `tf.nn.embedding_lookup`), then average them depth-wise into a single average embedding vector with shape `(batch_size, embedding_size)`. Do this in a method called `context_average_embedding` that takes in a `context` argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.4: Compute the loss\n",
    "We want to jointly train the logistic regression weights and the word embeddings using cross-entropy loss on the output of the logistic regression, but this is inefficient because of the large number of outputs (we'd need one logit for each word).\n",
    "\n",
    "Instead, approximate the per-example loss using noise-contrastive estimation by calling `tf.nn.nce_loss`, then return the mean loss for the batch.\n",
    "\n",
    "Since we will require many variables from our model to compute this loss, do this in a method called `loss` in your model class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.5: Operations find similar embeddings\n",
    "The **cosine similarity** between two embeddings $\\vec{x}$ and $\\vec{y}$ is the cosine of the angle between them, computed as:\n",
    "$$\n",
    "\\cos(\\vec{x}, \\vec{y}) = \\frac{\\vec{x} \\cdot \\vec{y}}{||\\vec{x}|| \\cdot ||\\vec{y}||}\n",
    "$$\n",
    "This is a more robust similarity measure than Euclidean distance, since it's invariant to scaling the embedding vectors by a constant.\n",
    "\n",
    "We'll use cosine similarity to find the most similar words to any given input word.\n",
    "The steps for doing this are:\n",
    " 1. Take a word's integer encoding as input, and compute its embedding with `tf.nn.embedding_lookup`.\n",
    " 2. Compute a \"similarity tensor\" which holds cosine similarity of this embedding with all of the word embeddings. This should result in a single tensor with shape `(n_words,).\n",
    " 3. Use `tf.nn.top_k` to find the top 8 similarities and their indices in the similarity tensor. These indices will be the integer encodings of the words with the most similar embeddings to the input. Return this tensor.\n",
    "\n",
    "Later, you can find the most similar words to a given input word by running the tensor that holds the top 8 indices, then using `tokenizer.index_word` on each of those indices.\n",
    "\n",
    "When debugging this, make sure that the most similar word to a given word is that word itself, with a cosine similarity of 1.\n",
    "\n",
    "Do this under a method called `compute_similar_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(tf.Module):\n",
    "    def __init__(self, n_words, embedding_size):\n",
    "        self.embeddings = tf.Variable(tf.initializers.glorot_uniform()(shape = (n_words, embedding_size), dtype = tf.float32), name = 'embeddings')\n",
    "        self.weights = tf.Variable(tf.initializers.glorot_uniform()(shape = (n_words, embedding_size), dtype = tf.float32), name = 'weights')\n",
    "        self.bias = tf.Variable(tf.zeros_initializer()(shape = (n_words,), dtype = tf.float32), name = 'bias')\n",
    "    \n",
    "    def context_average_embedding(self, context):\n",
    "        context_embeds = tf.nn.embedding_lookup(self.embeddings, context)\n",
    "        return tf.math.reduce_mean(context_embeds, axis = 1)\n",
    "    \n",
    "    def loss(self, x, y, num_negative_samples, n_words):\n",
    "        return tf.reduce_mean(tf.nn.nce_loss(weights = self.weights, biases = self.bias, labels = tf.expand_dims(y, axis = 1), \\\n",
    "            inputs = self.context_average_embedding(x), num_sampled = num_negative_samples, num_classes = n_words))\n",
    "    \n",
    "    def compute_similar_words(self, word):\n",
    "        embed = tf.nn.embedding_lookup(self.embeddings, word)\n",
    "        similarity_vec = []\n",
    "        for i in range(1,n_words):\n",
    "            other_embed = tf.nn.embedding_lookup(self.embeddings, i)\n",
    "            cos_sim = tf.reduce_sum(tf.multiply(embed, other_embed)) / (tf.norm(embed) * tf.norm(other_embed))\n",
    "            similarity_vec.append(cos_sim)\n",
    "        values, idces = tf.nn.top_k(tf.Variable(similarity_vec), k = 8)\n",
    "        return idces\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float64, numpy=array([ 1.        ,  0.99388373,  0.        , -1.        ])>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_similar_words(word):\n",
    "    embed = tf.nn.embedding_lookup(embedding_values, word)\n",
    "    similarity_vec = []\n",
    "    for i in range(4):\n",
    "        other_embed = tf.nn.embedding_lookup(embedding_values, i)\n",
    "        cos_sim = tf.reduce_sum(tf.multiply(embed, other_embed)) / (tf.norm(embed) * tf.norm(other_embed))\n",
    "        similarity_vec.append(cos_sim)\n",
    "    values, idces = tf.nn.top_k(tf.Variable(similarity_vec), k = 4)\n",
    "    return idces\n",
    "\n",
    "embedding_values = np.array([\n",
    "    [0, 0, 1],\n",
    "    [0, 0.1, 0.9],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, -1]\n",
    "])\n",
    "\n",
    "compute_similar_words(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Optimizer and gradients\n",
    "Make an optimizer (I used `tf.train.AdamOptimizer` with `learning_rate=1e-3`) and a `train` method with a `bool` switch to compute summaries. If this switch is true, add a summary scalar plot to plot the loss in TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.Adam()\n",
    "\n",
    "def train(model, x, y, i, switch):\n",
    "    with tf.GradientTape() as g:\n",
    "        loss = model.loss(x, y, n_negative_samples, n_words)\n",
    "    gradients = g.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    if switch:\n",
    "        with writer.as_default():\n",
    "            print(loss)\n",
    "            tf.summary.scalar('loss', loss, step = i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Train the model\n",
    "Use the same kind of training loop we've used before, repeatedly calling the method that applies gradient updates.\n",
    "\n",
    "The dataset is large, but the model doesn't do much work for each example.\n",
    "So, you might want to only run and save the summaries every 1000 batches or so to speed up training.\n",
    "\n",
    "In addition, every 1000 batches, for every word in `validation_words`, print the 8 most similar words by cosine similarity.\n",
    "\n",
    "Finally, use a `tf.train.Checkpoint()` to save the model's weights in `./logs`, which is necessary to visualize the embeddings in TensorBoard. Since we only care about the weights, initialize the checkpoint as `tf.train.Checkpoint(embedding=model.logistic_weights)`. Instead of `checkpoint.write(...)`, use `checkpoint.save('./logs/embedding.ckpt')` for a format TensorBoard can interpret in its embedding projector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "tf.Tensor(488.01633, shape=(), dtype=float32)\n",
      "Top 8 similar words to yes:\n",
      "see, devotchka, porno, leak, samoan, radiologist, instability, 5827b\n",
      "Top 8 similar words to small:\n",
      "straight, chiton, commanders, studiolo, scairt, fizzle, reconsidering, sharpish\n",
      "Top 8 similar words to thousand:\n",
      "pay, ion, demoted, ritchie, mirabile, tearjerker, containing, despa\n",
      "\n",
      "Epoch: 0\n",
      "tf.Tensor(305.8224, shape=(), dtype=float32)\n",
      "Top 8 similar words to yes:\n",
      "see, give, an, go, for, in, it's, will\n",
      "Top 8 similar words to small:\n",
      "straight, course, dr, an, not, have, it's, give\n",
      "Top 8 similar words to thousand:\n",
      "pay, mom, might, stop, place, things, anyone, guess\n",
      "\n",
      "Epoch: 0\n",
      "tf.Tensor(173.32785, shape=(), dtype=float32)\n",
      "Top 8 similar words to yes:\n",
      "see, give, for, go, in, it's, an, i'm\n",
      "Top 8 similar words to small:\n",
      "straight, course, dr, an, not, have, give, go\n",
      "Top 8 similar words to thousand:\n",
      "pay, mom, might, things, place, stop, anyone, us\n",
      "\n",
      "Epoch: 0\n",
      "tf.Tensor(150.58612, shape=(), dtype=float32)\n",
      "Top 8 similar words to yes:\n",
      "see, give, in, go, it's, for, i'm, a\n",
      "Top 8 similar words to small:\n",
      "straight, course, not, an, dr, give, have, go\n",
      "Top 8 similar words to thousand:\n",
      "pay, mom, might, place, stop, things, guess, anyone\n",
      "\n",
      "Epoch: 0\n",
      "tf.Tensor(86.098526, shape=(), dtype=float32)\n",
      "Top 8 similar words to yes:\n",
      "see, give, in, go, a, want, it's, will\n",
      "Top 8 similar words to small:\n",
      "straight, course, an, dr, not, give, much, go\n",
      "Top 8 similar words to thousand:\n",
      "pay, mom, stop, place, might, things, anyone, guess\n",
      "\n",
      "Epoch: 0\n",
      "tf.Tensor(70.752426, shape=(), dtype=float32)\n",
      "Top 8 similar words to yes:\n",
      "see, give, she, want, in, go, will, a\n",
      "Top 8 similar words to small:\n",
      "straight, course, an, much, dr, day, can't, really\n",
      "Top 8 similar words to thousand:\n",
      "pay, mom, might, stop, things, place, anyone, guess\n",
      "\n",
      "Epoch: 0\n",
      "tf.Tensor(31.217915, shape=(), dtype=float32)\n",
      "Top 8 similar words to yes:\n",
      "see, give, she, are, will, your, not, who\n",
      "Top 8 similar words to small:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-9137cbec9ff3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mwrd\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalidation_words\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Top 8 similar words to {}:'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m                 \u001b[0msim_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_similar_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwrd\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m                 print('{}, {}, {}, {}, {}, {}, {}, {}'.format(func(sim_words[0]),func(sim_words[1]),func(sim_words[2]), \\\n\u001b[0;32m     16\u001b[0m                     func(sim_words[3]),func(sim_words[4]),func(sim_words[5]),func(sim_words[6]),func(sim_words[7]),))\n",
      "\u001b[1;32m<ipython-input-9-3c335592eb68>\u001b[0m in \u001b[0;36mcompute_similar_words\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0msimilarity_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0mother_embed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m             \u001b[0mcos_sim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother_embed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother_embed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0msimilarity_vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcos_sim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\embedding_ops.py\u001b[0m in \u001b[0;36membedding_lookup_v2\u001b[1;34m(params, ids, max_norm, name)\u001b[0m\n\u001b[0;32m    368\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m   \"\"\"\n\u001b[1;32m--> 370\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0membedding_lookup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"div\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\embedding_ops.py\u001b[0m in \u001b[0;36membedding_lookup\u001b[1;34m(params, ids, partition_strategy, name, validate_indices, max_norm)\u001b[0m\n\u001b[0;32m    321\u001b[0m       \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m       \u001b[0mmax_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m       transform_fn=None)\n\u001b[0m\u001b[0;32m    324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\embedding_ops.py\u001b[0m in \u001b[0;36m_embedding_lookup_and_transform\u001b[1;34m(params, ids, partition_strategy, name, max_norm, transform_fn)\u001b[0m\n\u001b[0;32m    135\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         result = _clip(\n\u001b[1;32m--> 137\u001b[1;33m             array_ops.gather(params[0], ids, name=name), ids, max_norm)\n\u001b[0m\u001b[0;32m    138\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtransform_fn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m           \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mgather\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   4104\u001b[0m     \u001b[1;31m# TODO(apassos) find a less bad way of detecting resource variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4105\u001b[0m     \u001b[1;31m# without introducing a circular dependency.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4106\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4107\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4108\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36msparse_read\u001b[1;34m(self, indices, name)\u001b[0m\n\u001b[0;32m    643\u001b[0m       \u001b[0mvariable_accessed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m       value = gen_resource_variable_ops.resource_gather(\n\u001b[1;32m--> 645\u001b[1;33m           self._handle, indices, dtype=self._dtype, name=name)\n\u001b[0m\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariant\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_resource_variable_ops.py\u001b[0m in \u001b[0;36mresource_gather\u001b[1;34m(resource, indices, dtype, batch_dims, validate_indices, name)\u001b[0m\n\u001b[0;32m    542\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ResourceGather\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m         \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"batch_dims\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_dims\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 544\u001b[1;33m         \"validate_indices\", validate_indices, \"dtype\", dtype)\n\u001b[0m\u001b[0;32m    545\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Word2Vec(n_words, embedding_size)\n",
    "batches = 0\n",
    "\n",
    "func = lambda index: tokenizer.index_word[index.numpy()] \n",
    "\n",
    "for i in range(n_epochs):\n",
    "    for x, y in dataset:\n",
    "        if (batches % 1000 == 0):\n",
    "            print('')\n",
    "            print('Epoch: {}'.format(i))\n",
    "            train(model, x, y, batches, True)\n",
    "            for wrd in validation_words:\n",
    "                print('Top 8 similar words to {}:'.format(wrd))\n",
    "                sim_words = model.compute_similar_words(tokenizer.word_index[wrd])\n",
    "                print('{}, {}, {}, {}, {}, {}, {}, {}'.format(func(sim_words[0]),func(sim_words[1]),func(sim_words[2]), \\\n",
    "                    func(sim_words[3]),func(sim_words[4]),func(sim_words[5]),func(sim_words[6]),func(sim_words[7]),))\n",
    "                # for idx in sim_words:\n",
    "                    # print(tokenizer.index_word[idx.numpy()])\n",
    "        else:\n",
    "            train(model, x, y, batches, False)\n",
    "        batches += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Visualize the learned embeddings\n",
    "Run TensorBoard pointed at `./logs` and look in the Projector tab, then use the \"Load data\" button and select the `./logs/metadata.tsv` file we created earlier to load word labels. If projector does not show up, use the URL [`http://localhost:6006#projector`](http://localhost:6006#projector) depending on the port TensorBoard is using (mine uses 6006).\n",
    "\n",
    "Try typing some words into the search bar and see which words come up as most similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4 (optional): Generate analogies\n",
    "Try using embedding vector arithmetic to generate analogies.\n",
    "To do this:\n",
    " - Compute the embedding vector for several vectors using `tf.nn.embedding_lookup`\n",
    " - Do vector arithmetic on the computed embeddings\n",
    " - Find the most similar word embeddings by cosine similarity, then find the words that map to those embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
