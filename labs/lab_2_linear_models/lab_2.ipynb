{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Linear Models\n",
    "This lab, like the others that follow it, will be a small, largely self-guided project in building a machine learning model.\n",
    "You will write a logistic regression classifier, from scratch, to classify 28x28 pixel images of handwritten digits (0 - 9) by which digit appears in the image.\n",
    "This is [the famous MNIST dataset](http://yann.lecun.com/exdb/mnist/), which has 60,000 training examples and 10,000 test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorboard\n",
    "import watermark\n",
    "import matplotlib.pyplot as plt\n",
    "# Any other imports you need go here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: download the data\n",
    "Download the data as a CSV [here](https://pjreddie.com/projects/mnist-in-csv/) (the original data format is very unfriendly) and read the data format on the website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: load, understand, and preprocess the data\n",
    "Load the data from disk into four numpy arrays: x_train (feature vectors from the training set), y_train (labels from the training set), x_test (feature vectors from the test set), and y_test (labels from the test set).\n",
    "\n",
    "Then, do some simple preprocessing.\n",
    "Normalize your features by subtracting the mean and dividing by the range.\n",
    "This is less important for linear models, but is generally good practice and will be much more important with more complex models.\n",
    "Make sure you don't use any values computed from the test set, just the mean and range of the training set.\n",
    "\n",
    "Finally, display one of the training images, and print its correct label.\n",
    "\n",
    "Functions to look at:\n",
    " - `np.loadtxt`\n",
    " - `np.reshape`\n",
    " - `plt.imshow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 5.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading the dataset to a numpy array\n",
    "train_data = np.loadtxt('mnist_train.csv', delimiter = ',')\n",
    "test_data = np.loadtxt('mnist_test.csv', delimiter = ',')\n",
    "\n",
    "# Creating the x and y training/testing datasets\n",
    "x_train, y_train = train_data[:,1:], train_data[:,0]\n",
    "x_test, y_test = test_data[:,1:], test_data[:,0]\n",
    "\n",
    "# Normalization\n",
    "for feature in range(x_train.shape[1]):\n",
    "    feature_col = x_train[:,feature]\n",
    "    feature_mean = np.mean(feature_col)\n",
    "    feature_range = np.amax(feature_col) - np.amin(feature_col)\n",
    "    if (feature_range != 0):\n",
    "        x_test[:,feature] = (x_test[:,feature] - feature_mean)/feature_range\n",
    "        x_train[:,feature] = (feature_col - feature_mean)/feature_range\n",
    "    \n",
    "plt.figure()\n",
    "plt.imshow(np.reshape(x_train[0,:], (28,28)), cmap = 'gray')\n",
    "print('Label:',y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: build a data pipeline\n",
    "Make a pipeline that turns the dataset you have in numpy arrays into tensors that your model can use.\n",
    "The pipeline should also shuffle and batch the data (with some reasonable batch size; I used 64 because I'm training on a GPU but you might want to go lower).\n",
    "\n",
    "There are multiple ways to approach this.\n",
    "No matter what, you'll need two `tf.data.Dataset`s, one for train and one for test, and probably you'll do some transforms to that.\n",
    "I made datasets which did not use `repeat` -- you might instead repeat the data for as many epochs (full run-throughs of the dataset during training, ~5-10 for this assignment) as you plan to train for.\n",
    "\n",
    "Optionally, you might want to cache or prefetch data to prevent it from being loaded multiple times while the model is training and keep the model from needing to wait for data.\n",
    "\n",
    "Now might also be a good place to convert the labels to one-hot encoding (see below), though you don't need to.\n",
    "If you want, you could also do your data normalization on-the-fly here (but make sure it still does the same thing).\n",
    "\n",
    "I would also create `SummaryWriter`s for the train and test sets here.\n",
    "\n",
    "Functions to look at (not exhaustive):\n",
    " - `tf.data.Dataset.from_tensor_slices`\n",
    " - `tf.data.Dataset.shuffle`\n",
    " - `tf.data.Dataset.batch`\n",
    " - `tf.data.Dataset.repeat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "n_batches_per_epoch_train = x_train.shape[1] // batch_size\n",
    "n_batches_per_epoch_test = x_test.shape[1] // batch_size\n",
    "\n",
    "# Converting y_train to one-hot encoding\n",
    "y_train = tf.one_hot(tf.cast(y_train, tf.int32),10)\n",
    "y_test = tf.one_hot(tf.cast(y_test, tf.int32),10)\n",
    "\n",
    "# Turning arrays into tensors, shuffling, batching, and caching the data\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(x_train.shape[0]).batch(batch_size).cache()\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((x_test, y_test)).shuffle(x_test.shape[0]).batch(batch_size).cache()\n",
    "\n",
    "# Creating the SummaryWriters\n",
    "train_writer = tf.summary.create_file_writer('./dataset/train')\n",
    "test_writer = tf.summary.create_file_writer('./dataset/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: build a model graph\n",
    "This is where the actual model gets built.\n",
    "I'll give you the steps, but there's flexibility in how you implement each part.\n",
    "Make sure your design is clean, both for efficiency and ease of debugging.\n",
    "Names help divide the model into logical parts.\n",
    "\n",
    "Note that the first axis of every tensor will be the batch size.\n",
    "So, a tensor that in your model is a scalar will likely have a shape of (?), and a 10-element vector will have a shape of (?, 10).\n",
    "The \"?\" indicates that TensorFlow will treat this axis as being variable-length, since it can't infer it just from context.\n",
    "It may be helpful to switch your thinking back and forth from one view (no batches, scalars are scalars) when thinking about modeling to another (scalars come in batches) when writing code that changes shapes.\n",
    "\n",
    "If you're having difficulty debugging, try looking at your graph in TensorBoard, printing tensor objects to see their shapes, and running small parts of the graph while feeding values to certain tensors.\n",
    "\n",
    "Functions to look at throughout:\n",
    " - `tf.cast`\n",
    " - `tf.expand_dims`\n",
    " - `tf.squeeze`\n",
    " - arithmetic operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1: Class\n",
    "Write a `tf.Module` class that will be your logistic regression model. You may want to implement your graph operations in the `__call__` method for convenience.\n",
    "\n",
    "#### 3.1.1: Variables\n",
    "Logistic regression needs two variables, correctly shaped: a weight matrix and a vector of biases. \n",
    "\n",
    "#### 3.1.2: Logits\n",
    "Compute the inputs to the softmax functiton, the logits or \"unnormalized probabilities.\" The result should be, for each example in the batch, a vector of ten values (so the tensor has a shape like (?, 10)).\n",
    "\n",
    "#### 3.1.3: Softmax\n",
    "Apply the softmax function to the logits to obtain a vector of class probabilities.\n",
    "Again, there should be a probability for each class for each example.\n",
    "Do _not_ use helper functions to compute the softmax function, like `tf.nn.softmax` or `tf.contrib.layers.softmax`.\n",
    "Instead, stick to core operations like `tf.exp`.\n",
    "\n",
    "Two main reasons for this:\n",
    " 1. Better to learn without the API doing too much of the work for you; when doing model development it'll basically all be from scratch\n",
    " 2. I couldn't get `tf.nn.softmax` to run on a GPU, so I'd do it this way in practice too\n",
    "\n",
    "#### 3.1.4: Trace function\n",
    "Write a trace function to export your graph to TensorBoard (or decorate the `__call__` method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(tf.Module):\n",
    "    def __init__(self, name = None):\n",
    "        super().__init__(name)\n",
    "        \n",
    "        self.weights = tf.Variable(tf.initializers.glorot_uniform()(shape = (x_train.shape[1],10), dtype = tf.float64), name = 'weights')\n",
    "        self.bias = tf.Variable(tf.zeros_initializer()(shape=(10), dtype=tf.float64), name='bias')\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        logits = tf.math.add(tf.squeeze(tf.matmul(x, self.weights)),self.bias)\n",
    "        prob = tf.exp(logits)/tf.reshape(tf.reduce_sum(tf.exp(logits), axis = 1), (logits.shape[0],1))\n",
    "        return prob\n",
    "\n",
    "@tf.function        \n",
    "def trace_function(model, x):\n",
    "    model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2: Cross-entropy loss\n",
    "Compute the per-example cross-entropy loss $$L = -y \\cdot \\log p_\\text{model}(y)$$ using the probabilities and the correct label.\n",
    "\n",
    "Then, take the mean of the per-example losses to compute a per-batch loss.\n",
    "\n",
    "It may be convenient to convert the label to a one-hot vector: 10 elements, each of which is 0 except the place of the correct label.\n",
    "For instance, a label of \"3\" would be the vector $$[0, 0, 0, 1, 0, 0, 0, 0, 0, 0].$$\n",
    "\n",
    "Functions to look at:\n",
    " - `tf.one_hot`\n",
    " - `tf.math.log`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _loss(target, actual):\n",
    "    target = tf.cast(target, tf.float32)\n",
    "    actual = tf.cast(actual, tf.float32)\n",
    "    return tf.reduce_mean((tf.math.multiply(-target,tf.math.log(actual))))\n",
    "\n",
    "def accuracy(target, actual):\n",
    "    target_idx = tf.argmax(target)\n",
    "    actual_idx = tf.argmax(actual)\n",
    "    acc = len(tf.where(target_idx == actual_idx))/len(target)\n",
    "    return tf.constant(acc, dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3: The optimizer\n",
    "Add an optimizer (simple gradient descent is fine). You may need to play around with the learning rate to find one that works.\n",
    "\n",
    "Write a train and test method that each represent one step of training/testing. The train method should apply the optimizer to minimize the per-batch loss. Also, add a summary operation which saves the per-batch loss so you can plot it later.\n",
    "\n",
    "The model should predict the digit it assigns the highest probability. \n",
    "Add a tensor which represents what fraction of the batch the model predicted correctly (its accuracy, or average 0/1 loss), and a summary operation for accuracy.\n",
    "\n",
    "You should record these summaries for both training and testing.\n",
    "\n",
    "\n",
    "Functions to look at:\n",
    " - `tf.argmax`\n",
    " - `tf.equal`\n",
    " - `tf.reduce_mean`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.SGD(1e-1)\n",
    "\n",
    "def train(model, x, y, i):\n",
    "    with tf.GradientTape() as g:\n",
    "        loss = lambda: _loss(y, model(x))\n",
    "    acc = accuracy(y, model(x))\n",
    "        \n",
    "    optimizer.minimize(loss, model.trainable_variables)\n",
    "    \n",
    "    with train_writer.as_default():\n",
    "        tf.summary.scalar('loss', loss(), step = i)\n",
    "        tf.summary.scalar('accuracy', acc, step = i)\n",
    "        \n",
    "def test(model, x, y, i):\n",
    "    loss = _loss(y, model(x))\n",
    "    acc = accuracy(y, model(x))\n",
    "    with test_writer.as_default():\n",
    "        tf.summary.scalar('loss', loss, step = i)\n",
    "        tf.summary.scalar('accuracy', acc, step = i)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: train the model\n",
    "Make a training loop which iterates through the full training set multiple times, and, for each batch run the train method you wrote earlier. \n",
    "\n",
    "After each epoch, iterate over the test dataset and print the average test accuracy over the whole test set.\n",
    "Finally, use a `Checkpoint` to save the whole graph to disk.\n",
    "\n",
    "Remember to: \n",
    " - Run your trace method once to export your graph to TensorBoard\n",
    " - Give train and test methods step numbers (batch steps)\n",
    "\n",
    "Then, run training.\n",
    "You should expect the model to hit about 90% test-set accuracy.\n",
    "Not bad for a linear model!\n",
    "If it doesn't, it might indicate a bug in your code, or you might need to tune hyperparameters (e.g. batch size, learning rate).\n",
    "The accuracy should sharply jump after the first epoch, so you don't need to wait for the full model to train to know you have a bug. \n",
    "\n",
    "You might notice, looking at the training plots on TensorBoard, how close the training and test errors are.\n",
    "This suggests our model is probably underfitting, and a more powerful model will do better.\n",
    "\n",
    "<sup><sub>My model was showing 20% accuracy and it took over an hour to figure out I was actually just computing accuracy wrong :(</sub></sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Average Test Set Loss:  0.1168131\n",
      "Epoch:  1\n",
      "Average Test Set Loss:  0.11821086\n",
      "Epoch:  2\n",
      "Average Test Set Loss:  0.1188099\n",
      "Epoch:  3\n",
      "Average Test Set Loss:  0.118610226\n",
      "Epoch:  4\n",
      "Average Test Set Loss:  0.11871006\n",
      "Epoch:  5\n",
      "Average Test Set Loss:  0.119009584\n",
      "Epoch:  6\n",
      "Average Test Set Loss:  0.11950879\n",
      "Epoch:  7\n",
      "Average Test Set Loss:  0.11950879\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "train_batch = 0\n",
    "test_batch = 0\n",
    "\n",
    "for i in range(epochs):\n",
    "    for x, y in dataset_train:\n",
    "        if train_batch == 0:\n",
    "            tf.summary.trace_on(graph=True, profiler=True)\n",
    "            trace_function(model, x)\n",
    "            with train_writer.as_default():\n",
    "                tf.summary.trace_export(name = 'first training batch', step = 0, profiler_outdir = 'dataset')\n",
    "        train(model, x, y, train_batch)\n",
    "        train_batch += 1\n",
    "        \n",
    "    print('Epoch: ', i)\n",
    "    test_acc = []\n",
    "    for x, y in dataset_test:\n",
    "        test_acc.append(test(model, x, y, test_batch))\n",
    "        # Roughly align test batches with training batches\n",
    "        test_batch += 1\n",
    "        \n",
    "    print('Average Test Set Loss: ', np.mean(test_acc))\n",
    "    \n",
    "checkpoint = tf.train.Checkpoint(model=model)\n",
    "checkpoint.write('./checkpoints_lecture/model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: use the model for inference\n",
    "Pull one or more examples from the test set and display its image.\n",
    "Then, use your model to predict what digit the image is, print that, and print the true label (which hopefully matches).\n",
    "\n",
    "Remember you'll need to spin up a new instance of your model and checkpoint, then use `Checkpoint.restore()` to load the trained model before inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: visualizations\n",
    "Visualize, as an image (i.e. `plt.imshow`), the weight matrix for each of the digits.\n",
    "You might want to use the `vmin` and `vmax` arguments to put all of the matrices on the same scale. \n",
    "You'll need to load the weights again to save them to numpy arrays (by accessinging your class).\n",
    "\n",
    "The results are pretty interesting.\n",
    "The image should be brightest where the weights strongly indicate that digit, so you can see what's (linearly) characteristic of different digits -- I see a strong dark spot in the middle for zero, a distinctive curly tail on 2, and a consistent 3.\n",
    "Other digits (e.g. 8 and 9) have much weaker patterns, suggesting greater variation in how people draw them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
