{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Mapping with Fully-Connected Networks\n",
    "Blocks of dense layers are really good at memorizing patterns.\n",
    "This week we'll step outside the traditional \"prediction\" regime of machine learning and use a fully-connected network (FCN) to represent a 2-D scene in its weight by interpolating between known points.\n",
    "\n",
    "Throughout, you should be impressed by two (related) properties of dense neural nets:\n",
    " - They have high enough capacity to entirely memorize complex patterns\n",
    " - They are sufficiently nonlinear to build these patterns up from simple inputs (like (x, y) coordinates in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pointcloud mapping\n",
    "\n",
    "![full scene](./images/full_scene.png)\n",
    "\n",
    "### The problem\n",
    "Imagine there is a square (2-D) room with four walls and several square and circular objects in it.\n",
    "We put laser distance sensors around the room, which shoot rays that continue until they hit an object or a wall and report the distance, with limited angular and radial resolution.\n",
    "We want to use this limited data to produce a full 2-D reconstruction of the scene, filling in points the sensors haven't observed with likely values (in essence, interpolating).\n",
    "\n",
    "### The data\n",
    "We know that every ray that there's an object at the point each ray hit, and we also know that there's no object between the camera and the contact point.\n",
    "The data is provided as (x, y) pairs, where some points correspond to places we know are on object boundaries (shown in green above) and points we know are not inside objects (shown in red above).\n",
    "\n",
    "These points come in two data files: `data/positives.txt` for points on the edge of objects, and `data/negatives.txt` for points outside of objects.\n",
    "To generate these files, run `make_data.py`.\n",
    "\n",
    "(Check out the code if you want to see how the points are generated, but don't change anything!)\n",
    "\n",
    "### The model\n",
    "We want to store a full description of the scene in the weights of a neural network.\n",
    "To do this, we'll train a FCN to classify (x, y) points by whether or not they are the edge of a wall or object.\n",
    "Then, we can draw the scene by sampling (x, y) pairs and drawing the model's output.\n",
    "So, the network has \"memorized\" where the objects and walls are, and what their shapes are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Load the data\n",
    "Load the data using `np.loadtxt()` (default arguments should work), then convert it to a usable format with a feature array and label array, which is 1 if a point is the edge of an object and 0 if it is not.\n",
    "\n",
    "Functions to look at:\n",
    " - `np.loadtxt`\n",
    " - `np.concatenate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28556, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "pos = np.loadtxt('positives.txt')\n",
    "neg = np.loadtxt('negatives.txt')\n",
    "\n",
    "# Creating the labels\n",
    "pos_label = np.ones(pos.shape[0])\n",
    "neg_label = np.zeros(neg.shape[0])\n",
    "\n",
    "# Joining the separate datasets together, for both the datapoints and labels\n",
    "dataset = np.concatenate((pos, neg))\n",
    "labels = np.concatenate((pos_label, neg_label))\n",
    "\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Build a data pipeline\n",
    "Set up a `tf.data.Dataset` and a `tf.summary.SummaryWriter`.\n",
    "This time, there's no test set, just a single dataset. You may want to use the `repeat` method on the dataset for the number of desired epochs. If so, call it after caching so as not to store redundant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs = 100\n",
    "batch_size = 1000\n",
    "batches_per_epoch = dataset.shape[0] // batch_size\n",
    "\n",
    "# Creating a tensorflow dataset, shuffling, batching, and cacheing it\n",
    "tf_dataset = tf.data.Dataset.from_tensor_slices((dataset, labels)).shuffle(1000).batch(batch_size).cache()\n",
    "\n",
    "# Creating the SummaryWriter\n",
    "summary_writer = tf.summary.create_file_writer('./training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Build a model graph\n",
    "The model is a simple fully-connected neural network with four hidden layers, each with 64 units, and an output layer that performs binary classification with sigmoid activation.\n",
    "Recall that the input is just a pair (x, y).\n",
    "\n",
    "Functions to look at throughout:\n",
    " - `tf.cast`\n",
    " - `tf.expand_dims`\n",
    " - `tf.squeeze`\n",
    " - arithmetic operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1: Class to make dense layers\n",
    "It's tedious (and bad design) to explicitly write out every variable, matrix multiplication, etc.\n",
    "Instead, it's common to define classes that create one of a structure you expect to repeat in your model.\n",
    "\n",
    "Write a `tf.Module` class called `Dense`, which has its signature and scopes defined as a stub below.\n",
    "When called, it should add variables and operations to the graph which implement a single dense layer.\n",
    "\n",
    "If `do_activation` is True, it should apply ReLU activation to the computed values.\n",
    "To do this, you can use `tf.nn.relu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(tf.Module):\n",
    "    '''\n",
    "    Creates a dense layer module.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dim_input: int\n",
    "        Number of features in the input representation.\n",
    "    dim_output: int\n",
    "        Number of features in the output representation.\n",
    "        Equivalently, number of units in this layer.\n",
    "    do_activation: bool\n",
    "        Whether or not to apply ReLU activation.\n",
    "    postfix: string\n",
    "        Postfix on name scopes in this layer.\n",
    "        Used to simplify visualizations.\n",
    "    name: string\n",
    "        Name of layer.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    A tensor representing the activations of this layer.\n",
    "    '''\n",
    "    def __init__(self, dim_input, dim_output, do_activation=True, postfix='', name=None):\n",
    "        super().__init__(name=name)\n",
    "        with tf.name_scope('dense' + postfix):\n",
    "            self.weights = tf.Variable(tf.initializers.he_uniform()(shape = (dim_input, dim_output), dtype = tf.float64), name = 'weights-' + postfix)\n",
    "            self.bias = tf.Variable(tf.zeros_initializer()(shape = dim_output, dtype = tf.float64), name = 'bias-' + postfix)\n",
    "            self.do_activation = do_activation\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        output = tf.math.add(tf.matmul(x, self.weights), self.bias)\n",
    "        if self.do_activation:\n",
    "            activations = tf.nn.relu(output)\n",
    "            return activations\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2: Class to make neural network\n",
    "Use the `Dense` class to make four dense layers contained as fields in a `tf.Module` class called `FeedForward`.\n",
    "Each should have 64 units, the correct input dimensions, and a distinct (and meaningful) postfix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1: Compute logit\n",
    "Use the `Dense` class to make a final dense layer with `dim_output=1` to compute the final logit. You'll want to put this in a method called `logits` which should take in `self` and an input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2: Compute class probability for output\n",
    "Use `tf.sigmoid` to compute the probability that the input coordinate is a boundary point.\n",
    "We will not use this for the loss, just for the output. You'll want to put this in the `__call__` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3: Trace function\n",
    "Decorate the `__call__` method with `@tf.function` in order to run a model trace for TensorBoard. I have defined this method for you here, but will not in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(tf.Module):\n",
    "    def __init__(self, name=None):\n",
    "        self.dense1 = Dense(dataset.shape[1], 64, do_activation = True, postfix = 'dense_layer_1')\n",
    "        self.dense2 = Dense(64, 64, do_activation = True, postfix = 'dense_layer_2')\n",
    "        self.dense3 = Dense(64, 64, do_activation = True, postfix = 'dense_layer_3')\n",
    "        self.dense4 = Dense(64, 64, do_activation = True, postfix = 'dense_layer_4')\n",
    "    \n",
    "    def logits(self, x):\n",
    "        dense5 = Dense(x, 1, do_activation = False, postfix = 'dense_layer_final')\n",
    "        return dense5\n",
    "    \n",
    "    @tf.function\n",
    "    def __call__(self, x):\n",
    "        dense5 = self.logits(64)\n",
    "        prob = tf.sigmoid(dense5(self.dense4(self.dense3(self.dense2(self.dense1(x))))))\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3: Compute weighted cross-entropy loss\n",
    "There are about 33 times as many non-boundary coordinates as there are boundary coordinates.\n",
    "To prevent the network from just learning to predict that every coordinate is not a boundary, we should weight the loss function to consider boundary pixels more heavily.\n",
    "\n",
    "To do this, use `tf.nn.weighted_cross_entropy_with_logits()` on the logits computed before, plus the input label, and a positive weight of 20.\n",
    "Then, produce the mean loss for the batch with `tf.reduce_mean`. Put this in a `_loss` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _loss(target, actual):\n",
    "    loss = tf.nn.weighted_cross_entropy_with_logits(target, actual, 20)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4: Optimizer and gradients\n",
    "Make an optimizer (I used `tf.optimizers.SGD` with `learning_rate=2e-3` and `momentum=0.9`).\n",
    "Then write a train method (taking in a model, input, output, and step), that represents one step of training, where you explicitly compute the gradients in a variable called `gradients` (so we can plot them below) and make an operation that applies those gradients to the graph.\n",
    "\n",
    "Note: don't use `optimizer.minimize`, which will result in computing the gradients twice since we already have them from `GradientTape.gradient`. Add a summary scalar to plot loss in TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.1: Plot variables and gradients\n",
    "You don't need to do this for every model, but it's illustrative to do at least once. I've written the code for this below, with a pattern borrowed from http://matpalm.com/blog/viz_gradient_norms/. Put this code snippet under the same `with` block where you add your loss summary scalar.\n",
    "\n",
    "```\n",
    "for gradient, variable in zip(gradients, model.trainable_variables):\n",
    "    if variable.name and gradient is not None:\n",
    "        tf.summary.histogram('gradients/' + variable.name, tf.norm(gradient), step=i)\n",
    "        tf.summary.histogram('variables/' + variable.name, tf.norm(variable), step=i)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.SGD(2e-3, 0.9)\n",
    "\n",
    "def train(model, x, y, i):\n",
    "    with tf.GradientTape() as g:\n",
    "        loss = lambda: _loss(y, model(x))\n",
    "        \n",
    "    gradients = g.gradients(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    with summary_writer.as_default():\n",
    "        for gradient, variable in zip(gradients, model.trainable_variables):\n",
    "            if variable.name and gradient is not None:\n",
    "                tf.summary.histogram('gradients/' + variable.name, tf.norm(gradient), step=i)\n",
    "                tf.summary.histogram('variables/' + variable.name, tf.norm(variable), step=i)\n",
    "        tf.summary.scalar('loss', loss(), step = i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: train the model\n",
    "Same as last time, except there's no test set.\n",
    "You don't need to distinguish between epochs and steps here, but do run a full trace on the first batch to get the model graph in TensorBoard.\n",
    "\n",
    "I used 50 epochs, and got a final cross-entropy loss of about 0.4.\n",
    "You might not need to use this many epochs to get reasonable results, but more will get crisper results.\n",
    "\n",
    "This might take a while on CPUs, so if it's taking too long, think about:\n",
    " - Using Google Colaboratory\n",
    " - Increasing your batch size\n",
    " - Reducing the frequency with which you run summary ops\n",
    " \n",
    "Watching plots, histograms, etc in TensorBoard as the model trains can be fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Trace already enabled\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in converted code:\n\n    <ipython-input-27-453ed7a62aec>:14 __call__  *\n        dense5 = self.logits(64)\n    <ipython-input-27-453ed7a62aec>:9 logits  *\n        dense5 = Dense(x, 1, do_activation = False, postfix = 'dense_layer_final')\n    <ipython-input-32-7426818c3655>:27 __init__\n        self.weights = tf.Variable(tf.initializers.he_uniform()(shape = (dim_input, dim_output), dtype = tf.float64), name = 'weights-' + postfix)\n    C:\\Users\\Ammu\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py:260 __call__\n        return cls._variable_v2_call(*args, **kwargs)\n    C:\\Users\\Ammu\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py:254 _variable_v2_call\n        shape=shape)\n    C:\\Users\\Ammu\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py:65 getter\n        return captured_getter(captured_previous, **kwargs)\n    C:\\Users\\Ammu\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py:502 invalid_creator_scope\n        \"tf.function-decorated function tried to create \"\n\n    ValueError: tf.function-decorated function tried to create variables on non-first call.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-df5665e53771>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtrain_batch\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrace_on\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprofiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m             \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtrain_writer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                 \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrace_export\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'first training batch'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprofiler_outdir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'training'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    630\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    633\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2360\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2362\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2363\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2702\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2703\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2704\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2705\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   2591\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2592\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2593\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   2594\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2595\u001b[0m         \u001b[1;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    976\u001b[0m                                           converted_func)\n\u001b[0;32m    977\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 978\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    979\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 439\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    440\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mbound_method_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   3209\u001b[0m     \u001b[1;31m# However, the replacer is still responsible for attaching self properly.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3210\u001b[0m     \u001b[1;31m# TODO(mdan): Is it possible to do it here instead?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3211\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3212\u001b[0m   \u001b[0mweak_bound_method_wrapper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbound_method_wrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    966\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 968\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    969\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    970\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in converted code:\n\n    <ipython-input-27-453ed7a62aec>:14 __call__  *\n        dense5 = self.logits(64)\n    <ipython-input-27-453ed7a62aec>:9 logits  *\n        dense5 = Dense(x, 1, do_activation = False, postfix = 'dense_layer_final')\n    <ipython-input-32-7426818c3655>:27 __init__\n        self.weights = tf.Variable(tf.initializers.he_uniform()(shape = (dim_input, dim_output), dtype = tf.float64), name = 'weights-' + postfix)\n    C:\\Users\\Ammu\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py:260 __call__\n        return cls._variable_v2_call(*args, **kwargs)\n    C:\\Users\\Ammu\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py:254 _variable_v2_call\n        shape=shape)\n    C:\\Users\\Ammu\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\variables.py:65 getter\n        return captured_getter(captured_previous, **kwargs)\n    C:\\Users\\Ammu\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py:502 invalid_creator_scope\n        \"tf.function-decorated function tried to create \"\n\n    ValueError: tf.function-decorated function tried to create variables on non-first call.\n"
     ]
    }
   ],
   "source": [
    "model = FeedForward()\n",
    "train_batch = 0\n",
    "\n",
    "for i in range(epochs):\n",
    "    for x, y in tf_dataset:\n",
    "        if train_batch == 0:\n",
    "            tf.summary.trace_on(graph = True, profiler = True)\n",
    "            model(x)\n",
    "            with train_writer.as_default():\n",
    "                tf.summary.trace_export(name = 'first training batch', step = 0, profiler_outdir = 'training')\n",
    "        train(model, x, y, train_batch)\n",
    "        train_batch += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: Plot the model\n",
    "Let's see what the network learned.\n",
    "Generate 100 x values and 100 y values, each in the range (-1, 1).\n",
    "Then compute the value of `probability` (the sigmoid output of your model) using every coordinate in that grid (every pairwise combination of x and y values), and plot a \"map\" of those probabilities using `plt.scatter()` (check out the `c` argument)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6: TensorBoard plots\n",
    "This time, we have some more interesting TensorBoard plots than last time.\n",
    "Run TensorBoard, and look at:\n",
    " - The model graph in Graphs\n",
    " - The loss over time in Scalars\n",
    " - The weights, biases, and gradients over time in Distributions and Histograms\n",
    " \n",
    "The loss should decrease over time, and eventually stabilize (with some noise).\n",
    "\n",
    "The gradients for weights and biases may not go to zero, and your weights and biases may not look like they're converging.\n",
    "This is because of \"bias drift\" where multiple possible settings of weights and biases have the same loss value.\n",
    "To fix this, we could add a term to the loss which penalizes the model slightly based on the magnitude of the bias terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 7: Again, but in Keras\n",
    "Make a Keras Sequential model which is equivalent to the one we've trained in TensorFlow.\n",
    "Train it on the same dataset, and again plot its output in the same way. \n",
    "\n",
    "Hints:\n",
    " - This won't take nearly as long as the previous sections\n",
    " - You don't need to use `keras.utils.to_categorical` since this is binary classification.\n",
    " - The appropriate loss function is 'binary_crossentropy'\n",
    " - To implement the same loss weighting, use `class_weight={0: 1.0, 1:20.0}` in `model.fit()`\n",
    " - You can get the outputs of the model over the entire grid at once with `model.predict()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1: Build a model\n",
    "Use `keras.models.Sequential` to build the same FCN as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2: Define an optimizer and compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3: Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4: Plot the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
